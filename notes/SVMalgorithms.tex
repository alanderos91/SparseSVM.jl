\documentclass[11pt]{article}
\usepackage{amsbsy,amsthm,amsmath,amssymb}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newcommand{\svskip}{\vspace{1.75mm}}
\newcommand{\mvskip}{\vspace{.25in}}
\newcommand{\lvskip}{\vspace{.5in}}
\def\E{\mathop{\rm E\,\!}\nolimits}
\def\Var{\mathop{\rm Var}\nolimits}
\def\Cov{\mathop{\rm Cov}\nolimits}
\def\den{\mathop{\rm den}\nolimits}
\def\midd{\mathop{\,|\,}\nolimits}
\def\sgn{\mathop{\rm sgn}\nolimits}
\def\vec{\mathop{\rm vec}\nolimits}
\def\sinc{\mathop{\rm sinc}\nolimits}
\def\curl{\mathop{\rm curl}\nolimits}
\def\div{\mathop{\rm div}\nolimits}
\def\tr{\mathop{\rm tr}\nolimits}
\def\len{\mathop{\rm len}\nolimits}
\def\diag{\mathop{diag}\nolimits}
\def\dist{\mathop{\rm dist}\nolimits}
\def\cond{\mathop{\rm cond}\nolimits}
\def\prox{\mathop{\rm prox}\nolimits}
\def\argmin{\mathop{\rm argmin}\nolimits}
\def\amp{\mathop{\;\:}\nolimits}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bc}{\boldsymbol{c}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\be}{\boldsymbol{e}}
\newcommand{\fb}{\boldsymbol{f}}
\newcommand{\bg}{\boldsymbol{g}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bi}{\boldsymbol{i}}
\newcommand{\bj}{\boldsymbol{j}}
\newcommand{\bk}{\boldsymbol{k}}
\newcommand{\bl}{\boldsymbol{l}}
\newcommand{\bm}{\boldsymbol{m}}
\newcommand{\bn}{\boldsymbol{n}}
\newcommand{\bo}{\boldsymbol{o}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bq}{\boldsymbol{q}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bC}{\boldsymbol{C}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\bE}{\boldsymbol{E}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bG}{\boldsymbol{G}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bJ}{\boldsymbol{J}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bM}{\boldsymbol{M}}
\newcommand{\bN}{\boldsymbol{N}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bxi}{\boldsymbol{\xi}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\bXi}{\boldsymbol{\Xi}}
\newcommand{\bPi}{\boldsymbol{\Pi}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bUpsilon}{\boldsymbol{\Upsilon}}
\newcommand{\bPhi}{\boldsymbol{\Phi}}
\newcommand{\bPsi}{\boldsymbol{\Psi}}
\newcommand{\bOmega}{\boldsymbol{\Omega}}

\pagenumbering{gobble}

\title{Support Vector Machines Algorithms}
\author{Kenneth Lange
\\
\\
\\
Departments of Biomathematics, \\
Human Genetics, and Statistics \\
University of California \\
Los Angeles, CA 90095\\
Phone: 310-206-8076 \\
E-mail klange@ucla.edu \\
\\
\\
\\
\\
Research supported in part by USPHS grants GM53275 and HG006139.}

\begin{document}

\maketitle
%\newpage
%\begin{abstract}

\noindent  \\ \\
\lvskip
%\noindent {\bf Key Words:} 

%\end{abstract}

\newpage
\pagenumbering{arabic}

\baselineskip=20pt

\section*{\center Potential Algorithms for Sparse SVM}

In SVM the loss $L(\bbeta)=\sum_{i=1}^m \max\{1-y_i \bx_i^t\bbeta,0\}^2$ is convenient. We assume here that the last entry $\beta_p$ of $\bbeta$ is the intercept and the last entry $x_{ip}$ of each $\bx_i$ is accordingly $1$.  Several algorithms are possible for minimizing $L(\bbeta)$. In MM optimization, the squared hinge function $\max\{1-u,0\}^2$ is majorized by $(u_k-u)^2$ for $u_k \ge 1$ and by $(1-u)^2$ for $u_k < 1$. Hence, the loss is majorized by
\begin{eqnarray*}
L(\bbeta,\alpha) & \le & \sum_{i=1}^m
\begin{cases} (1-y_i \bx_i^t \bbeta)^2 & y_i \bx_i^t \bbeta_k \le 1 \\
(y_i \bx_i\bbeta_k - y_i \bx_i^t \bbeta)^2 & y_i \bx_i^t \bbeta_k > 1 .\end{cases}
\end{eqnarray*} 
In standard SVM the ridge penalty $\rho \sum_{j=1}^{p-1}\beta_j^2$ is added to the loss to give the objective. For sparse estimations one can replace the ridge by 
$\rho\dist(\bbeta,S)^2$, where $S$ is the set of sparse vectors $\bbeta$ with at most $r$ nontrivial components. The sparsity penalty is majorized by $\rho \|\bbeta-P_{S}(\bbeta_k)\|^2$, where $P_{S}$ denotes projection onto $S$. It follows that minimization of the surrogate reduces to iteratively re-weighted least squares. When $y_i \in \{-1,1\}$, the surrogate becomes
\begin{eqnarray*}
L(\bbeta,\alpha) & \le & \sum_{i=1}^m
\begin{cases} (y_i - \bx_i^t \bbeta)^2 & y_i \bx_i^t \bbeta_k \le 1 \\
(\bx_i^t \bbeta_k - \bx_i^t \bbeta^2 & y_i \bx_i^t \bbeta_k > 1 . \end{cases}
\end{eqnarray*}
If $\bX$ is the matrix with $i$th row $\bx_i^t$ and $\by$ is the vector with $i$th component $y_i$, then the surrogate can be expressed as
\begin{eqnarray*}
\left\|\begin{pmatrix}\bz_k \\\bp_{k} \end{pmatrix}-
\begin{pmatrix} \bX \\ \bD_{\rho}\end{pmatrix}\bbeta
\right\|^2,
\end{eqnarray*}
where $\bz_k$ has $i$th component $y_i$ when $y_i \bx_i^t\bbeta_k \le 1$
and $y_i \bx_i^t\bbeta_k$ otherwise. 
The ridge penalty version requires $\bp_{k} = \bzero$ and $\bD_{\rho} = \begin{pmatrix} \sqrt{\rho} \bI & \bzero_{p-1} \\ \bzero_{p-1}^{t} & 0 \end{pmatrix}$.
For sparse estimation we set $\bp_k = \sqrt{\rho} P_S(\bbeta_k)$ and $\bD_{\rho} = \sqrt{\rho} \bI$.

Steepest descent and Newton's method are also possible. The gradient of the loss of equals the sum $-\sum_{i=1}^m 1_{\{1-y_i\bx_i^t\bbeta > 0\}}(1-y_i\bx_i^t\bbeta)y_i\bx_i$, and the Hessian is the sum $\sum_{i=1}^m 1_{\{1-y_i\bx_i^t\bbeta > 0\}}\bx_i\bx_i^t$, ignoring the rare case where some $1-y_i\bx_i^t\bbeta = 0$. The Hessian is valuable even in steepest descent because it is instrumental in determining a optimal step size.
The Hessian of the MM surrogate $\bX^t\bX= \sum_{i=1}^m \bx_i\bx_i^t$ serves as an approximation to the true Hessian. 
 
Coordinate descent can be implemented by keeping track of the scalars $w_i = 1-y_i\bx_i^t\bbeta$. Let $f_j(u)$ be the objective with $\beta_j$ replaced by $\beta_j+u$. Then $f_j(u)=\frac{1}{2}\sum_i (w_i-y_ix_{ij}u)_+^2+1_{\{j \ne p\}}\frac{\rho}{2}(\beta_j+u)^2$ up to a constant. Furthermore, $\frac{1}{2}f_j'(u)=-\sum_i (w_i-y_ix_{ij}u)_+y_ix_{ij}+1_{\{j \ne p\}}\rho(\beta_j+u)$. One can update each $\beta_j$ by a convex version of bisection. Consider the supporting line inequality $f_j(u)\ge f_j(0)+f_j'(0)u$. If $f_j'(0)>0$ and we move right, then we increase $f_j(u)$. Likewise, if $f_j'(0)<0$ and we move left, then we again increase $f_j(u)$. Thus, in a bisection strategy, we replace the upper bound by $0$ when $f_j'(0)>0$ and the lower bound by $0$ when $f_j'(0)<0$. The case $f_j'(0)=0$ signifies a minimum. Of course, every time we change $\beta_j$ to $\beta_j+u$, we must change each $w_i$ to $w_i-y_ix_{ij}u$.

Coordinate descent as just described is cumbersome to implement. An easier choice
is to update each parameter by an MM algorithm. The dominating quadratic for
parameter $j$ has second derivative $\sum_i x_{ij}^2 +\rho$. The intercept case omits the penalty term $\rho$. 

Of the algorithms discussed, coordinate descent takes the least storage. Steepest descent makes slightly greater demands on memory. Newton's method is infeasible in high dimensions. The MM algorithm makes more modest memory demands than Newton's method, but it does require solving a least squares problem at each iteration.  Steepest descent is the fastest and most accurate of the four methods. This contention is born out in the following Julia code.

\section*{\center Preliminary Code}

\begin{verbatim}
using LinearAlgebra, IterativeSolvers

function newton(X::Matrix{T}, y::Vector{T}, rho::T, 
  tol::T) where T <: Real
#
  (m, n) = size(X)
  (obj, old, iters, q) = (zero(T), zero(T), 0, n -1)
  grad = zeros(n)
  hess = zeros(n, n)
  b = zeros(n)
  for iter = 1:200
    iters = iters + 1
    obj = rho * (norm(b[1:n])^2 - b[n]^2)
    Xb = X * b
    grad .= zero(T)
    hess = rho * [Matrix{T}(I, q, q) zeros(q); zeros(n)']
    for i = 1:m
      c = y[i] * Xb[i]
      if c < one(T)
        d = one(T) - c
        obj = obj + d^2
        grad = grad - (y[i] * d) .* (X[i, :])
        hess = hess + X[i, :] * X[i, :]'
      end
    end
    b = b - hess \ grad
#     if iter <= 10 || mod(iter, 10) == 0 
#       println("iter = ",iter," obj = ",obj)
#     end
    if abs(old - obj) < tol * (old + one(T))
#     if norm(grad) < tol
      break
    else
      old = obj
    end  
  end
  println(iters,"  ",obj)
  return b
end

function steepest(X::Matrix{T}, y::Vector{T}, rho::T, 
  tol::T) where T <: Float64
#
  (m, n) = size(X)
  (obj, old, iters) = (zero(T), zero(T), 0)
  (b, grad, increment) = (zeros(T, n), zeros(T, n), zeros(T, n))
  a = zeros(T, m)
  Xb = zeros(T, m)
  mul!(Xb, X, b)
  for iter = 1:1000
    iters = iters + 1
    @. grad = rho * b # penalty contribution
    grad[n] = zero(T)
    for i = 1:m
      a[i] = - y[i] * max(one(T) - y[i] * Xb[i], zero(T))
    end
    BLAS.gemv!('T', 1.0, X, a, 0.0, grad)
    s = norm(grad)^2 # optimal step size
    mul!(Xb, X, grad)
    t = norm(Xb)^2
    s = s / (t + rho * (norm(grad)^2 - grad[n]^2))
    @. increment = - s * grad
    for step = 0:3 # step halving
      @. b = b + increment
      obj = rho *(norm(b)^2 - b[n]^2)
      mul!(Xb, X, b)
      for i = 1:m
        obj = obj + max(one(T) - y[i] * Xb[i], zero(T))^2
      end
      if obj < old
        break
      else
        @. b = b - increment
        @. increment = 0.5 * increment
      end
    end
#     if iter <= 10 || mod(iter, 10) == 0 
#       println("iter = ",iter," obj = ",obj)
#     end
    if  abs(old - obj) < tol * (old + one(T)) # norm(grad) < tol
      break
    else
      old = obj
    end  
  end
  println(iters,"  ",obj)
  return b
end

function MMsvm(X::Matrix{T}, y::Vector{T}, rho::T, 
  tol::T) where T <: Real
#
  (m, n) = size(X)
  (obj, old, iters) = (zero(T), zero(T), 0)
  z = zeros(T, m + n - 1)
  A = [X; sqrt(rho) * Matrix{T}(I, n - 1, n - 1) zeros(T, n - 1)]
  (b, c) = (zeros(T, n), zero(T))
  Xb = zeros(m)
  QR = qr(A)
  for iter = 1:1000
    iters = iters + 1
    obj = rho *(norm(b)^2 - b[n]^2)
    mul!(Xb, X, b)
    for i = 1:m
      c = y[i] * Xb[i]
      if c > one(T)
        z[i] = y[i] * c
      else
        z[i] = y[i]
        obj = obj + (one(T) - c)^2
      end
    end
    ldiv!(b, QR, z)
#    b = A \ z
#    lsqr!(b, A, z) 
#     if iter <= 10 || mod(iter, 10) == 0 
#       println("iter = ",iter," obj = ",obj)
#     end
    if abs(old - obj) < tol * (old + one(T))
      break
    else
      old = obj
    end  
  end
  print(iters,"  ",obj)
  return b
end

function MMcoordinate(X::Matrix{T}, y::Vector{T}, rho::T,
  tol::T) where T <: Real
#
  (m, n) = size(X)
  (obj, old, iters) = (zero(T), zero(T), 0)
  (b, w, c) = (zeros(T, n), ones(T, m), zeros(T, n))
  @. c = rho # curvatures
  c[n] = zero(T) 
  for j = 1:n
    for i = 1:m
      c[j] = c[j] + X[i, j]^2
    end
  end
  for iter = 1:1000
    iters = iters + 1
    for j = 1:n # update parameter j
      u = rho * b[j]
      if j == n u = zero(T) end
      for i = 1:m
        if w[i] > zero(T) u = u - w[i] * y[i] * X[i, j] end
      end
      u = -u / c[j]
      b[j] = b[j] + u
      for i = 1:m
        w[i] = w[i] - y[i] * X[i, j] * u
      end
    end      
    obj = rho *(norm(b)^2 - b[n]^2)
    for i = 1:m
      if w[i] > zero(T) obj = obj + w[i]^2 end
    end
#     if iter <= 10 || mod(iter, 10) == 0 
#       println("iter = ",iter," obj = ",obj)
#     end
    if abs(old - obj) < tol * (old + one(T))
      break
    else
      old = obj
    end  
  end
  print(iters,"  ",obj)
  return b
end

function run_example(m, n, rho, tol)
  X = randn(m, n);
  X = [X ones(m)];
  y = randn(m);
  @. y = sign(y);
#  @time b = newton(X, y, rho, tol);
  @time b = steepest(X, y, rho, tol);
  @time b = MMsvm(X, y, rho, tol);
  @time b = MMcoordinate(X, y, rho, tol);
end

(rho, tol, m, n) = (1.0, 1.0e-5, 1000, 500);
for i = 1:10
  println("trial = ",i)
  run_example(m, n, rho, tol)
end
\end{verbatim}

\begin{thebibliography}{99}
\end{thebibliography}

\end{document}
